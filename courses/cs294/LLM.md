### Nvidia
2013.03, Nvidia to stack up DRAM on future "Volta" GPUs, [link](https://www.theregister.com/2013/03/19/nvidia_gpu_roadmap_computing_update/)
### Outrageously Large Neural Networks
2017.01, Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, [paper](http://arxiv.org/abs/1701.06538)
### Learning to Generate Reviews
2017.04, Learning to Generate Reviews and Discovering Sentiment, [paper](http://arxiv.org/abs/1704.01444)
### Sparse Transformers
2019.04, Generating Long Sequences with Sparse Transformers, [paper](http://arxiv.org/abs/1904.10509)
### Scaling Laws
2020.01, Scaling Laws for Neural Language Models, [paper](http://arxiv.org/abs/2001.08361)
### Longformer
2020.12, Longformer: The Long-Document Transformer, [paper](http://arxiv.org/abs/2004.05150)
### Math Word Problems
2021.11, Training Verifiers to Solve Math Word Problems, [paper](http://arxiv.org/abs/2110.14168)
### Scratchpads
2021.11, Show Your Work: Scratchpads for Intermediate Computation with Language Models, [paper](http://arxiv.org/abs/2112.00114)
### Retrieval
2022.02, Improving language models by retrieving from trillions of tokens, [paper](http://arxiv.org/abs/2112.04426)
### InstructGPT
2022.03, Training language models to follow instructions with human feedback, [paper](http://arxiv.org/abs/2203.02155)
### Chinchilla
2022.03, Training Compute-Optimal Large Language Models, [paper](http://arxiv.org/abs/2203.15556)
### FlashAttention
2022.06, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, [paper](http://arxiv.org/abs/2205.14135)
### Scaling Laws vs Architectures
2022.07, Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?, [paper](http://arxiv.org/abs/2207.10551)
### S4
2022.08, Efficiently Modeling Long Sequences with Structured State Spaces, [paper](http://arxiv.org/abs/2111.00396)
### Long Video Generation
2022.09, Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer, [paper](http://arxiv.org/abs/2204.03638)
### Imagen Video
2022.10, Imagen Video: High Definition Video Generation with Diffusion Models, [paper](http://arxiv.org/abs/2210.02303)
### Zero-Shot Reasoning
2023.01, Large Language Models are Zero-Shot Reasoners, [paper](http://arxiv.org/abs/2205.11916)
### LLaMA
2023.02, LLaMA: Open and Efficient Foundation Language Models, [paper](http://arxiv.org/abs/2302.13971)
### RedPajama
2023.04, RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens, [link](https://www.together.ai/blog/redpajama)
### Let's Verify
2023.05, Let's Verify Step by Step, [paper](http://arxiv.org/abs/2305.20050)
### FlashAttention-2
2023.07, FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, [paper](http://arxiv.org/abs/2307.08691)
### Attention Is All You Need
2023.08, Attention Is All You Need, [paper](http://arxiv.org/abs/1706.03762)
### Transformer-BW
2023.08, Blockwise Parallel Transformer for Large Context Models, [paper](http://arxiv.org/abs/2305.19370)
### Ring Transformer
2023.11, Ring Attention with Blockwise Transformers for Near-Infinite Context, [paper](http://arxiv.org/abs/2310.01889)
### Mamba
2023.12, Mamba: Linear-Time Sequence Modeling with Selective State Spaces, [paper](http://arxiv.org/abs/2312.00752)
### DeepSeek
2024.01, DeepSeek LLM: Scaling Open-Source Language Models with Longtermism, [paper](http://arxiv.org/abs/2401.02954)
### Mixtral of Experts
2024.01, Mixtral of Experts, [paper](http://arxiv.org/abs/2401.04088)
### Pandora
2024.02, Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning, [paper](http://arxiv.org/abs/2402.08416)
### Linear Attention
2024.02, Simple linear attention language models balance the recall-throughput tradeoff, [paper](http://arxiv.org/abs/2402.18668)
### Griffin
2024.02, Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models, [paper](http://arxiv.org/abs/2402.19427)
### GaLore
2024.03, GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection, [paper](http://arxiv.org/abs/2403.03507)
### RFM-1
2024.03, Introducing RFM-1: Giving robots human-like reasoning capabilities, [link](https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/)
### Tokenizer for Visual Gen
2024.03, Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation, [paper](http://arxiv.org/abs/2310.05737)
### KVQuant
2024.04, KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization, [paper](http://arxiv.org/abs/2401.18079)
### RedPajama-Data
2024.04, togethercomputer/RedPajama-Data, [link](https://github.com/togethercomputer/RedPajama-Data)
### GaLore
2024.04, GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection, [link](https://arxiv.org/abs/2403.03507)
### DeepSeek-V2
2024.05, DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model, [paper](http://arxiv.org/abs/2405.04434)
### Chain of Thoughtlessness
2024.05, Chain of Thoughtlessness: An Analysis of CoT in Planning, [paper](http://arxiv.org/abs/2405.04776)
### Arithmetic Transformers
2024.05, Transformers Can Do Arithmetic with the Right Embeddings, [paper](http://arxiv.org/abs/2405.17399)
### ReAct Prompting
2024.05, Researchers at Arizona State University Evaluates ReAct Prompting: The Role of Example Similarity in Enhancing Large Language Model Reasoning, [link](https://www.marktechpost.com/2024/05/28/researchers-at-arizona-state-university-evaluates-react-prompting-the-role-of-example-similarity-in-enhancing-large-language-model-reasoning/)
### MAP-Neo
2024.06, MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series, [paper](http://arxiv.org/abs/2405.19327)