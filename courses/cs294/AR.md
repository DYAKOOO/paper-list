




### MADE 

arXiv  2015.06, MADE: Masked Autoencoder for Distribution Estimation, <a href="http://arxiv.org/abs/1502.03509"> </a>http://arxiv.org/abs/1502.03509

### Gated Pixel CNN 

arXiv  2016.06, Conditional Image Generation with PixelCNN Decoders, <a href="http://arxiv.org/abs/1606.05328"> </a>http://arxiv.org/abs/1606.05328


### PixelRNN and PixelCNN Introduced 

arXiv  2016.08, Pixel Recurrent Neural Networks, <a href="http://arxiv.org/abs/1601.06759"> </a>http://arxiv.org/abs/1601.06759

### WaveNet

arXiv  2016.09, WaveNet: A Generative Model for Raw Audio, <a href="http://arxiv.org/abs/1609.03499"> </a>http://arxiv.org/abs/1609.03499

### PixelCNN++
arXiv  2017.01, PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications, <a href="http://arxiv.org/abs/1701.05517"> </a>http://arxiv.org/abs/1701.05517

### Concrete Distribution 

arXiv  2017.03, The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, <a href="http://arxiv.org/abs/1611.00712"> </a>http://arxiv.org/abs/1611.00712

### PixelCNN Super resolution 

arXiv  2017.03, Pixel Recursive Super Resolution, <a href="http://arxiv.org/abs/1702.00783"> </a>http://arxiv.org/abs/1702.00783


### GreyScale PixelCNN 

arXiv  2017.07, PixelCNN Models with Auxiliary Variables for Natural Image Modeling, <a href="http://arxiv.org/abs/1612.08185"> </a>http://arxiv.org/abs/1612.08185


### Gumbel-Softmax 

arXiv  2017.08, Categorical Reparameterization with Gumbel-Softmax, <a href="http://arxiv.org/abs/1611.01144"> </a>http://arxiv.org/abs/1611.01144

### PixelSNAIL

arXiv  2017.12, PixelSNAIL: An Improved Autoregressive Generative Model, <a href="http://arxiv.org/abs/1712.09763"> </a>http://arxiv.org/abs/1712.09763

### VQ-VAE 

arXiv  2018.05, Neural Discrete Representation Learning, <a href="http://arxiv.org/abs/1711.00937"> </a>http://arxiv.org/abs/1711.00937


### Sparse Transformer 

arXiv  2019.04, Generating Long Sequences with Sparse Transformers, <a href="http://arxiv.org/abs/1904.10509"> </a>http://arxiv.org/abs/1904.10509

### Scaling AR Video Models 

arXiv  2020.02, Scaling Autoregressive Video Models, <a href="http://arxiv.org/abs/1906.02634"> </a>http://arxiv.org/abs/1906.02634

### Natural Image Manipulation for Autoregressive Models using Fisher Scores

arXiv  2020.05, Natural Image Manipulation for Autoregressive Models Using Fisher Scores, <a href="http://arxiv.org/abs/1912.05015"> </a>http://arxiv.org/abs/1912.05015



### VIT - Vision Transformers 

arXiv  2021.06, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, <a href="http://arxiv.org/abs/2010.11929"> </a>http://arxiv.org/abs/2010.11929


### VG-GAN 

arXiv  2021.06, Taming Transformers for High-Resolution Image Synthesis, <a href="http://arxiv.org/abs/2012.09841"> </a>http://arxiv.org/abs/2012.09841

###  State Space Model (SSM)

arXiv  2022.08, Efficiently Modeling Long Sequences with Structured State Spaces, <a href="http://arxiv.org/abs/2111.00396"> </a>http://arxiv.org/abs/2111.00396

### MAGVIT 

arXiv  2023.04, MAGVIT: Masked Generative Video Transformer, <a href="http://arxiv.org/abs/2212.05199"> </a>http://arxiv.org/abs/2212.05199

### Transformers - Currently the most used Autoregressive model. (V7)

arXiv  2023.08, Attention Is All You Need, <a href="http://arxiv.org/abs/1706.03762"> </a>http://arxiv.org/abs/1706.03762

### MAGVIT-2

arXiv  2024.03, Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation, <a href="http://arxiv.org/abs/2310.05737"> </a>http://arxiv.org/abs/2310.05737

